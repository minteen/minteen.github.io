{"meta":{"title":"这里空荡荡的...","subtitle":"","description":"","author":"Minteen","url":"https://minteen.github.io","root":"/"},"pages":[{"title":"关于","date":"2024-01-27T15:58:30.766Z","updated":"2024-01-27T15:58:30.766Z","comments":false,"path":"about/index.html","permalink":"https://minteen.github.io/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"404 Not Found：该页无法显示","date":"2024-01-27T15:58:30.765Z","updated":"2024-01-27T15:58:30.765Z","comments":false,"path":"/404.html","permalink":"https://minteen.github.io/404.html","excerpt":"","text":""},{"title":"分类","date":"2024-01-27T15:58:30.768Z","updated":"2024-01-27T15:58:30.768Z","comments":false,"path":"categories/index.html","permalink":"https://minteen.github.io/categories/index.html","excerpt":"","text":""},{"title":"书单","date":"2024-01-27T15:58:30.767Z","updated":"2024-01-27T15:58:30.767Z","comments":false,"path":"books/index.html","permalink":"https://minteen.github.io/books/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2024-01-27T15:58:30.768Z","updated":"2024-01-27T15:58:30.768Z","comments":true,"path":"links/index.html","permalink":"https://minteen.github.io/links/index.html","excerpt":"","text":""},{"title":"标签","date":"2024-01-27T15:58:30.770Z","updated":"2024-01-27T15:58:30.770Z","comments":false,"path":"tags/index.html","permalink":"https://minteen.github.io/tags/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2024-01-27T15:58:30.769Z","updated":"2024-01-27T15:58:30.769Z","comments":false,"path":"repository/index.html","permalink":"https://minteen.github.io/repository/index.html","excerpt":"","text":""}],"posts":[{"title":"图神经网络的数学基础","slug":"人工智能/从深度学习到图神经网络/第二章：图神经网络的数学基础","date":"2024-02-16T12:28:01.054Z","updated":"2024-02-16T12:30:12.043Z","comments":true,"path":"2024/02/16/人工智能/从深度学习到图神经网络/第二章：图神经网络的数学基础/","permalink":"https://minteen.github.io/2024/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E4%BB%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/","excerpt":"","text":"矩阵论基础 标量和向量 转置、内积、正交 阿达玛乘积(Hadamard Product)：x⊙yx\\odot yx⊙y 主成分分析（Principal Component Analysis，PCA） 向量范数 ∥x∥1=∑i=1n∣xi∣\\parallel x\\parallel_1=\\sum_{i=1}^n\\lvert x_i\\rvert∥x∥1​=∑i=1n​∣xi​∣ ∥x∥2=∑i=1nxi2\\parallel x\\parallel_2=\\sqrt{\\sum_{i=1}^nx_i^2}∥x∥2​=∑i=1n​xi2​​，欧几里得范数 ∥x∥∞=max⁡∣xi∣\\parallel x\\parallel_\\infty=\\max\\begin{vmatrix}x_i\\end{vmatrix}∥x∥∞​=max∣∣∣​xi​​∣∣∣​ 向量的夹角与余弦相似度 矩阵与张量 特征值 平方分解 特征分解：矩阵的特征分解(Eigen Decomposition)又称谱分解(Spectral Decomposition) 图论基础 图的表示 有向图/无向图 权值图 邻接矩阵与关联矩阵 邻域与度 二分图（二部图，偶图） 符号图 图的遍历 图的同构与异构 图同构(Graph Isomorphism)：图G=(V,E)G=(V,E)G=(V,E)和图G′=(V′,E′)G&#x27;=(V&#x27;,E&#x27;)G′=(V′,E′)是同构的，当且仅当存在一个从GGG到G′G&#x27;G′的映射σ\\sigmaσ，使得GGG中任意两个节点uuu和vvv相连接，即(u,e)∈E(u,e)\\in E(u,e)∈E，当且仅当G′G&#x27;G′中对应的两个节点σ(u)\\sigma(u)σ(u)和σ(v)\\sigma(v)σ(v)相连接，即(σ(u),σ(v))∈E′(\\sigma(u),\\sigma(v))\\in E&#x27;(σ(u),σ(v))∈E′，同构可记作G≃G′G\\simeq G&#x27;G≃G′。 异构图：图中的节点类型或关系类型多于一种。 图的途径、迹与路 图的连通性 节点的中心性 度中心性：cd(νi)=d(νi)=∑j=1NAi,jc_d\\left(\\nu_i\\right)=d\\left(\\nu_i\\right)=\\sum_{j=1}^NA_{i,j}cd​(νi​)=d(νi​)=∑j=1N​Ai,j​，归一化DCνi=d(νi)∣V∣−1DC_{\\nu_i}=\\frac{d\\left(\\nu_i\\right)}{|V|-1}DCνi​​=∣V∣−1d(νi​)​ 特征向量中心性 介数中心度 谱图论基础 拉普拉斯矩阵的来源 拉普拉斯矩阵的性质 拉普拉斯矩阵的谱分解 拉普拉斯矩阵的归一化","categories":[{"name":"从深度学习到图神经网络","slug":"从深度学习到图神经网络","permalink":"https://minteen.github.io/categories/%E4%BB%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}],"tags":[]},{"title":"第一章：图上的深度学习","slug":"人工智能/从深度学习到图神经网络/第一章：图上的深度学习","date":"2023-07-16T10:55:45.000Z","updated":"2024-02-16T12:30:18.820Z","comments":true,"path":"2023/07/16/人工智能/从深度学习到图神经网络/第一章：图上的深度学习/","permalink":"https://minteen.github.io/2023/07/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E4%BB%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E5%9B%BE%E4%B8%8A%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"人工智能与深度学习 深度学习的发展 机器学习中的特征工程：传统机器学习性能的好坏很大程度上取决于特征工程。工程师能成功提取有用特征的前提条件通常是其已经在特定领域摸爬滚打多年，对领域知识有非常深入的理解。 深度学习中的端到端（End-to-End）概念：整个学习流程并不进行人为的子问题划分，而是完全交给深度学习模型，使其直接学习从原始输入到期望输出的映射。 人工智能的底层逻辑 符号主义、联结主义、行为主义，三者都是研究事物之间的关系。 符号主义：用公理和逻辑体系搭建一套人工智能系统。 联结主义：主张模仿人类的神经元联结机制来实现人工智能。 行为主义：智能取决于感知和行动，环境的反馈有助于智能的提升。 深度学习的研究应从以感知智能为主，逐步向基于认知的逻辑推理方向演进。认知图谱=知识图谱+认知推理+逻辑表达 图神经网络 此图非彼图 图像Image：图像是基于点阵的，点阵是一种基于格子(Grid)的数据，其表达依赖于像素(Pixel) 图Graph：由若干个节点(Node)及连接节点的边(Edge)所构成的，用于表达不同实体间的关系 图的应用：作为一种高效描述实体间关系的数据结构，很多涉及关系的计算问题，都可以转化为一个面向图的计算问题。比如，在社交网络分析、推荐网络分析、疾病传播探究、基因表达网络分析、细胞相似性分析等领域 图神经网络的本质 图神经网络是机器学习的一种特定方式，是神经网络在图数据应用上的一个自然延伸。 所谓机器学习，在形式上，可近似等同于通过统计或推理的方法寻找一个有关特定输入和预期输出的功能函数f，即 Y≈f(X)Y\\approx f(X)Y≈f(X)，图神经网络也是如此。 图数据处理面临的挑战 欧氏空间难表示图 传统的深度学习模型非常擅长处理简单而有序的序列数据或栅格数据，如常见的图像、音频、语音和文本等。 文本数据和语音数据具有一定的时序性，属于1D栅格数据，这种序列结构与循环神经网络(Recurrent Neural Network,RNN)的“品性”非常契合，因此RNN和LSTM在在具备时序特征的1D栅格数据处理上具有天然优势。 静态的图像属于2D栅格数据，动态的视频属于3D栅格数据（或者说是具有时序特征的2D栅格数据），它们非常适配于卷积神经网络(Convolutional Neural Network,CNN)模型。 图的表示要复杂得多，图之间的相似性也很难在欧氏空间中衡量。因此，人们不得不寻求在非欧氏空间中来定义图。 图表达无固定格式：图中的关系并没有固定的表达方式，这就导致很多完全等价的图同构(Graph Isomorphism)难以被发现。 图可视化难理解 图数据不符合独立同分布：独立同分布是机器学习领域很重要的假设。这个假设意味着，如果假设训练数据和测试数据是满足相同分布的，那么通过训练数据获得的模型（比如，拟合一个决策平面）能够在测试集上获得很好的预测效果。 图神经网络的应用层面 针对图数据，图神经网络的预测主要体现在三个层面：节点层面(Node-level)、边层面(Edge-level)、图层面(Graph-level)。 节点预测 在节点层面的预测任务主要包括分类任务和回归任务。分类任务和回归任务本质一样，类似于定性和定量的区别。 “知己知彼”，即（采样）聚合邻居节点的信息，并和自身节点信息进行融合。当邻居信息汇集到本地节点后，就可以利用传统的方法（如深度学习）来做节点的分类或回归 边预测 推荐系统 药物研发：研究药物-靶标相互作用、蛋白质-蛋白质相互作用和药物-药物相互作用 图预测 预测整个图的属性，将标签和整个图像关联起来 图神经网络的发展简史 早期的图神经网络 首先将图结构数据“映射”为更简单的表示，如边列表法，邻接列表法。 再用特定算法加以处理，如搜索算法（BFS，DFS）、最短路径（迪杰斯特拉算法）、生成树算法（Prim）以及聚类算法。 弊端： 可能丢失了图的结构信息，如节点间的拓扑关系 需要一定置信水平的关于图的先验知识 图卷积神经网络 卷积神经网络CNN开始在计算机视觉和图像处理领域“乘风破浪”，取得了很多令人瞩目的成就。CNN的显著特点：局部连接、权值共享、多层处理（通过“卷积”操作，提取多尺度局部空间特征，并将其组合，构建具有高度表达性的表示形式） 将CNN运用于Graph的理论基础： 图天然具备局部连通性质 与传统的谱图理论(Spectral Graph Theory)相比，权值共享能大幅降低训练成本 图本身具有层次属性，多层结构能够捕捉不同抽象级别的特征 将CNN运用于Graph的理论障碍 经典的CNN模型只适用于文本（1D栅格）、图像（2D栅格）等欧氏空间下的数据，而图通常被认为是非欧氏空间下的数据。 CNN中的卷积和汇聚，在图数据中难以定义 图卷积神经网络(Graph Convolutional Neural Network,GCN) 面向图数据的基于谱域(Spectral-Based)的卷积神经网络 拉普拉斯矩阵可视为图结构数据的一种表达方式。通过对拉普拉斯算子做特征分解，然后只取低阶的向量，可达到“低通滤波”的效果。 图卷积运算的一个极简版操作就是取中心节点及其邻居节点的特征平均值 基于频域的卷积，在计算时，需要在处理整个图数据的同时承担矩阵分解时的极高计算复杂度，当图数据的规模较大时，这种策略难以奏效 图表示学习 图表示学习旨在学习通过低维向量表征图的节点、边、子图。在图分析领域中，传统的机器学习方法往往依赖于手工设计的特征，这些方法灵活度较低且计算开销较大。于是图嵌入表示应运而生。 图嵌入(Graph Embedding)：将图中的节点以低维稠密向量的形式进行表达，要求原始图中相似的节点在低维表达空间中也是接近的。得到的低维表达向量可以用来完成下游任务，如节点分类、链接预测或重构原始图等。 DeepWalk：第一个基于表示学习的图嵌入方法，借鉴Word2Vec算法。简单且易于实现，但它也有一些局限性，比如它无法捕获节点之间的多跳关系。 随机游走（Random Walk） 从图中的某个节点开始，随机选择一个邻居节点进行移动。 重复该过程，每次都从前一个节点的邻居中随机选择一个节点，直到达到预定的路径长度。 生成节点序列 对图中的每个节点都执行多次随机游走，以生成大量的节点序列。 这些序列可以被视为“句子”，其中的节点可以被视为“单词”。 训练节点向量 使用节点序列，采用类似于Word2Vec中的Skip-Gram模型来训练节点的向量表示。 目标是使得图中相邻的节点在向量空间中具有相似的向量表示。 Line： Node2Vec：采用的是一种有偏(Bias)的随机游走，它可以看作DeepWalk算法的一种扩展，通过DFS（深度优先搜索）和BFS（广度优先搜索）策略，Node2vec方法能更好地发现图的结构（社区）信息，从而获得优于DeepWalk算法的性能。 图卷积的简化 谱域图卷积 GraphSAGE（Graph Sample and Aggregation） 通过采样中心节点的邻居，并对这些邻居的特征进行聚合，以此来更新中心节点的嵌入。 这个过程可以迭代多次，每次迭代都可以考虑更远的邻居 传统GCN通常需要处理整个图或者至少是一个较大的子图。 图神经网络的计算模块 传播模块(Propagation Module)。传播模块用于在节点之间传播信息，以便聚合邻居节点的特征信息和拓扑信息。在传播模块中，卷积算子和递归算子通常用于融合来自邻居的信息。在传播模块中，可采用跳跃连接(Skip Connection)结构，跳跃连接也称为“短路连接”，在残差网络(ResNet)中得到广泛应用。跳跃连接能从前向节点中直接抽取历史数据，从而防止信息在传播过程中过度衰减。 采样模块(Sampling Module)。GNN通常会聚合前一层节点的邻域信息，从而生成当前节点的表征。如果我们的汇聚操作涉及若干GNN层，那么计算牵涉的邻居节点数将会随着深度的增加而呈指数级增长。因此，人们通过采样技术来解决这种“邻居爆炸”的问题。当图比较大时，由于计算负载和存储的限制，也需要用采样模块对图的规模进行缩减。采样模块通常与传播模块配合使用。 汇聚模块(Pooling Module)。所谓汇聚（亦称池化），可形象地将其理解为“群众代表”，它用局部区间的代表性统计信息（比如最大值、最小值或平均值）来代替整个局部区间，从而减少待处理的数据量。通过层层汇聚，经过KKK轮消息传递，当前节点能够提取更为全局的信息 在每一个隐含层，我们都可以使用卷积/循环算子、采样算子和池化算子，或者添加若干汇聚模块用以减少数据量和提取更为高层的抽象信息。为了获得更好的性能，GNN层通常会堆叠多个隐含层。 图神经网络的分类 按照图类型的不同： 有向图 异构图(Heterogeneous Graphs) 带边信息的图(Edge-Informative Graph) 动态图(Dynamic Graphs) 训练过程中采取的局部数据处理方式的不同，主要存在于： 邻居节点采样 感受野(Receptive Field)（（如控制变量法）） 数据增强(Data Augmentation) 非监督训练方式（如利用不同的Boosting方法） 按照信息传播方式的不同： 展望 健壮性强的、有意识的、能决策的新一代人工智能 GNN可视为符号主义和联结主义的交叉融合。 它通过图节点之间的消息传递来捕获图的依赖性，这种依赖性可以用来传递因果关系、逻辑推理等（这是符号主义的体现）。 汇聚信息之后，对于处理信息的方式，中心节点完全可以用普通的神经网络来进行处理（这是联结主义的体现）。 近年来，网络架构、优化技术和并行计算等技术的发展推动了GNN的研究进程。特别是基于图神经网络的变体，如图卷积网络(GCN)、图注意网络(GAT)、门控图神经网络(GGNN)的提出，让GNN在许多任务上都取得了令人瞩目的成就。","categories":[{"name":"从深度学习到图神经网络","slug":"从深度学习到图神经网络","permalink":"https://minteen.github.io/categories/%E4%BB%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2022-12-31T17:12:12.000Z","updated":"2024-02-16T11:25:18.242Z","comments":true,"path":"2023/01/01/hello-world/","permalink":"https://minteen.github.io/2023/01/01/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"从深度学习到图神经网络","slug":"从深度学习到图神经网络","permalink":"https://minteen.github.io/categories/%E4%BB%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}],"tags":[]}