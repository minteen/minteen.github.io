{"meta":{"title":"这里空荡荡的...","subtitle":"","description":"","author":"Minteen","url":"https://minteen.github.io","root":"/"},"pages":[{"title":"关于","date":"2024-01-27T15:58:30.766Z","updated":"2024-01-27T15:58:30.766Z","comments":false,"path":"about/index.html","permalink":"https://minteen.github.io/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"书单","date":"2024-01-27T15:58:30.767Z","updated":"2024-01-27T15:58:30.767Z","comments":false,"path":"books/index.html","permalink":"https://minteen.github.io/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2024-01-27T15:58:30.768Z","updated":"2024-01-27T15:58:30.768Z","comments":false,"path":"categories/index.html","permalink":"https://minteen.github.io/categories/index.html","excerpt":"","text":""},{"title":"404 Not Found：该页无法显示","date":"2024-01-27T15:58:30.765Z","updated":"2024-01-27T15:58:30.765Z","comments":false,"path":"/404.html","permalink":"https://minteen.github.io/404.html","excerpt":"","text":""},{"title":"友情链接","date":"2024-01-27T15:58:30.768Z","updated":"2024-01-27T15:58:30.768Z","comments":true,"path":"links/index.html","permalink":"https://minteen.github.io/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2024-01-27T15:58:30.769Z","updated":"2024-01-27T15:58:30.769Z","comments":false,"path":"repository/index.html","permalink":"https://minteen.github.io/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2024-01-27T15:58:30.770Z","updated":"2024-01-27T15:58:30.770Z","comments":false,"path":"tags/index.html","permalink":"https://minteen.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"","slug":"计算机基础/Python/4.函数式编程","date":"2024-02-16T12:37:50.469Z","updated":"2024-02-16T12:37:57.717Z","comments":true,"path":"2024/02/16/计算机基础/Python/4.函数式编程/","permalink":"https://minteen.github.io/2024/02/16/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/Python/4.%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"2. python 函数","slug":"计算机基础/Python/2.python函数","date":"2024-02-16T12:36:47.969Z","updated":"2024-02-16T12:37:32.113Z","comments":true,"path":"2024/02/16/计算机基础/Python/2.python函数/","permalink":"https://minteen.github.io/2024/02/16/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/Python/2.python%E5%87%BD%E6%95%B0/","excerpt":"","text":"默认参数 必选参数在前，默认参数在后 只有与默认参数不符的学生才需要提供额外的信息 默认参数必须指向不变对象：默认参数L也是一个变量，它指向对象[]，每次调用该函数，如果改变了L的内容，则下次调用时，默认参数的内容就变了，不再是函数定义时的[]了。 123456def add_end(L=[]): L.append(&#x27;END&#x27;) return Ladd_end()add_end()# 结果L=[&#x27;END&#x27;, &#x27;END&#x27;] 不变对象str、None：不变对象一旦创建，对象内部的数据就不能修改，这样就减少了由于修改数据导致的错误。此外，由于对象不变，多任务环境下同时读取对象不需要加锁，同时读一点问题都没有。我们在编写程序时，如果可以设计一个不变对象，那就尽量设计成不变对象。 可变参数 12345678910111213def calc_1(*numbers): # 可以传入任意个参数，包括0个参数 sum = 0 for n in numbers: sum = sum + n * n return sumdef calc_2(numbers): # 必须传入tuple或list sum = 0 for n in numbers: sum = sum + n * n return sum *：可变参数 如果想传入list或tuple，可以在其前面加上*号，如 calc_2(*li) 关键字参数 传入0个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个dict 1234567def person(name, age, **kw): print(&#x27;name:&#x27;, name, &#x27;age:&#x27;, age, &#x27;other:&#x27;, kw)person(&#x27;Adam&#x27;, 45, gender=&#x27;M&#x27;, job=&#x27;Engineer&#x27;)extra = &#123;&#x27;city&#x27;: &#x27;Beijing&#x27;, &#x27;job&#x27;: &#x27;Engineer&#x27;&#125;person(&#x27;Jack&#x27;, 24, **extra) 参数组合 递归函数 所有的递归函数都可以写成循环的方式，但循环的逻辑不如递归清晰。 递归函数需要注意防止栈溢出，每当进入一个函数调用，栈就会加一层栈帧，每当函数返回，栈就会减一层栈帧。 尾递归优化：无论调用多少次，都只占用一个栈帧，不会出现栈溢出的情况。遗憾的是，大多数编程语言没有针对尾递归做优化，Python解释器也没有做优化，所以，即使把递归函数改成尾递归方式，也会导致栈溢出。","categories":[{"name":"python基础","slug":"python基础","permalink":"https://minteen.github.io/categories/python%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"1. python基础","slug":"计算机基础/Python/1.python基础","date":"2024-02-16T12:34:12.349Z","updated":"2024-02-16T12:36:11.431Z","comments":true,"path":"2024/02/16/计算机基础/Python/1.python基础/","permalink":"https://minteen.github.io/2024/02/16/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/Python/1.python%E5%9F%BA%E7%A1%80/","excerpt":"","text":"python简介 优点 适合开发： 网络应用，包括网站、后台服务等等 日常需要的小工具，包括系统管理员需要的脚本任务等等 把其他语言开发的程序再包装起来，方便使用 缺点： 运行速度慢，解释型语言在执行时会一行一行地翻译成CPU能理解的机器码，而C程序是运行前直接编译成CPU能执行的机器码 代码不能加密，C语言不用发布源代码，只需要把编译后的机器码 python解释器 CPython：官方自带 IPython：CPython用&gt;&gt;&gt;作为提示符，而IPython用In [序号]:作为提示符。 PyPy：采用JIT技术，对Python代码进行动态编译（注意不是解释），所以可以显著提高Python代码的执行速度。PyPy与CPython略有差异。 Jyphon：Jython是运行在Java平台上的Python解释器，可以直接把Python代码编译成Java字节码执行。 IronPython：和Jython类似，只不过IronPython是运行在微软.Net平台上的Python解释器，可以直接把Python代码编译成.Net的字节码。 数据类型、变量 十六进制：用0x前缀和0-9，a-f表示，例如：0xff00，0xa5b4c3d2。 _分割：对于很大的数字（十进制/十六进制），允许使用_进行分割。 科学计数法：把10用e替代，1.23×1091.23\\times10^91.23×109就是1.23e9，或者12.3e8，0.000012可以写成1.2e-5，等等。 转义字符\\：字符\\本身也要转义，所以\\\\表示的字符就是\\ '''...'''的格式表示多行内容 用r''表示''内部的字符串默认不转义 动态语言 常量：用大写表示常量是一种习惯性的用法，没有硬性保障 /除法计算结果是浮点数，即使是两个整数恰好整除，结果也是浮点数 //，称为地板除，两个整数的除法仍然是整数 字符串是不可变 字符编码 三种字符编码 ASCII编码：1个字节； Unicode编码：2个字节。对于在ASCII中存在字符，第一个字节全部填0，第二个字节同ASCII编码； UTF-8：将Unicode字符根据不同的数字大小编码成1-6个字节，节省空间。ASCII编码实际上可以被看成是UTF-8编码的一部分。 字符⇔\\Leftrightarrow⇔编码 ord()函数获取单个字符的整数表示 chr()函数把编码转换为对应的单个字符 string.encode('ascii'/utf-8) '\\u4e2d\\u6587'：使用编码表示字符串 对于Linux系统，需要告诉系统这是一个Python可执行环境，告诉Python解释器文件编码。在Windows系统下可省略 12#!/usr/bin/env python3# -*- coding: utf-8 -*- 格式化 %占位符： 用法：'Hi, %s, you have $%d.' % ('Michael', 1000000)。若只有一个变量，括号可以省略。 %d：整数 %f：浮点数 %s：字符串 %x：十六进制整数用 转义：%%来表示一个% format() 用传入的参数依次替换字符串内的占位符&#123;0&#125;、&#123;1&#125; e.g. 'Hello, &#123;0&#125;, 成绩提升了 &#123;1:.1f&#125;%'.format('小明', 17.125) f-string list -1做索引，直接获取最后一个元素；以此类推，可以获取倒数第2个、倒数第3个 append(element) insert(索引:int, element) pop() pop(索引:int) 替换：直接赋值 tuple 一旦初始化就不能修改，所以代码更安全 当只有1个元素时，为了与数学公式小括号区分，必须在添加逗号，如 t = (1,) tuple元素不变，是指tuple中的指针不变！如 t = (li:list)，li列表中的元素可以改变 控制语句 if：只要x是非零数值、非空字符串、非空list等，就判断为True，否则为False。 input：返回的数据类型是str match：类似于C中的switch，但是可以匹配多个值 case 'A': case x if x &lt; 10: case 11 | 12 | 13 | 14 | 15: case ['gcc', file1, *files]: case _: _表示匹配到其他任何情况 dict Python内置了字典：dict的支持，dict全称dictionary，在其他语言中也称为map，使用键-值（key-value）存储，具有极快的查找速度。 哈希函数 查找和插入的速度极快，不会随着key的增加而变慢（与list相反） 需要占用大量的内存，内存浪费多（与list相反） 避免key不存在的错误： 通过in判断key是否存在 get()方法：如果key不存在，可以返回None pop(key) 添加：直接添加 set 重复元素在set中自动被过滤 add(key)方法：可以添加元素到set中 remove(key)方法：可以删除元素","categories":[{"name":"python基础","slug":"python基础","permalink":"https://minteen.github.io/categories/python%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"第二章：图神经网络的数学基础","slug":"人工智能/从深度学习到图神经网络/第二章：图神经网络的数学基础","date":"2024-02-16T12:28:01.054Z","updated":"2024-02-16T12:32:34.550Z","comments":true,"path":"2024/02/16/人工智能/从深度学习到图神经网络/第二章：图神经网络的数学基础/","permalink":"https://minteen.github.io/2024/02/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E4%BB%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/","excerpt":"","text":"矩阵论基础 标量和向量 转置、内积、正交 阿达玛乘积(Hadamard Product)：x⊙yx\\odot yx⊙y 主成分分析（Principal Component Analysis，PCA） 向量范数 ∥x∥1=∑i=1n∣xi∣\\parallel x\\parallel_1=\\sum_{i=1}^n\\lvert x_i\\rvert∥x∥1​=∑i=1n​∣xi​∣ ∥x∥2=∑i=1nxi2\\parallel x\\parallel_2=\\sqrt{\\sum_{i=1}^nx_i^2}∥x∥2​=∑i=1n​xi2​​，欧几里得范数 ∥x∥∞=max⁡∣xi∣\\parallel x\\parallel_\\infty=\\max\\begin{vmatrix}x_i\\end{vmatrix}∥x∥∞​=max∣∣∣​xi​​∣∣∣​ 向量的夹角与余弦相似度 矩阵与张量 特征值 平方分解 特征分解：矩阵的特征分解(Eigen Decomposition)又称谱分解(Spectral Decomposition) 图论基础 图的表示 有向图/无向图 权值图 邻接矩阵与关联矩阵 邻域与度 二分图（二部图，偶图） 符号图 图的遍历 图的同构与异构 图同构(Graph Isomorphism)：图G=(V,E)G=(V,E)G=(V,E)和图G′=(V′,E′)G&#x27;=(V&#x27;,E&#x27;)G′=(V′,E′)是同构的，当且仅当存在一个从GGG到G′G&#x27;G′的映射σ\\sigmaσ，使得GGG中任意两个节点uuu和vvv相连接，即(u,e)∈E(u,e)\\in E(u,e)∈E，当且仅当G′G&#x27;G′中对应的两个节点σ(u)\\sigma(u)σ(u)和σ(v)\\sigma(v)σ(v)相连接，即(σ(u),σ(v))∈E′(\\sigma(u),\\sigma(v))\\in E&#x27;(σ(u),σ(v))∈E′，同构可记作G≃G′G\\simeq G&#x27;G≃G′。 异构图：图中的节点类型或关系类型多于一种。 图的途径、迹与路 图的连通性 节点的中心性 度中心性：cd(νi)=d(νi)=∑j=1NAi,jc_d\\left(\\nu_i\\right)=d\\left(\\nu_i\\right)=\\sum_{j=1}^NA_{i,j}cd​(νi​)=d(νi​)=∑j=1N​Ai,j​，归一化DCνi=d(νi)∣V∣−1DC_{\\nu_i}=\\frac{d\\left(\\nu_i\\right)}{|V|-1}DCνi​​=∣V∣−1d(νi​)​ 特征向量中心性 介数中心度 谱图论基础 拉普拉斯矩阵的来源 拉普拉斯矩阵的性质 拉普拉斯矩阵的谱分解 拉普拉斯矩阵的归一化","categories":[{"name":"从深度学习到图神经网络","slug":"从深度学习到图神经网络","permalink":"https://minteen.github.io/categories/%E4%BB%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}],"tags":[]},{"title":"第一章：图上的深度学习","slug":"人工智能/从深度学习到图神经网络/第一章：图上的深度学习","date":"2023-07-16T10:55:45.000Z","updated":"2024-02-16T12:30:43.212Z","comments":true,"path":"2023/07/16/人工智能/从深度学习到图神经网络/第一章：图上的深度学习/","permalink":"https://minteen.github.io/2023/07/16/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E4%BB%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E5%9B%BE%E4%B8%8A%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"人工智能与深度学习 深度学习的发展 机器学习中的特征工程：传统机器学习性能的好坏很大程度上取决于特征工程。工程师能成功提取有用特征的前提条件通常是其已经在特定领域摸爬滚打多年，对领域知识有非常深入的理解。 深度学习中的端到端（End-to-End）概念：整个学习流程并不进行人为的子问题划分，而是完全交给深度学习模型，使其直接学习从原始输入到期望输出的映射。 人工智能的底层逻辑 符号主义、联结主义、行为主义，三者都是研究事物之间的关系。 符号主义：用公理和逻辑体系搭建一套人工智能系统。 联结主义：主张模仿人类的神经元联结机制来实现人工智能。 行为主义：智能取决于感知和行动，环境的反馈有助于智能的提升。 深度学习的研究应从以感知智能为主，逐步向基于认知的逻辑推理方向演进。认知图谱=知识图谱+认知推理+逻辑表达 图神经网络 此图非彼图 图像Image：图像是基于点阵的，点阵是一种基于格子(Grid)的数据，其表达依赖于像素(Pixel) 图Graph：由若干个节点(Node)及连接节点的边(Edge)所构成的，用于表达不同实体间的关系 图的应用：作为一种高效描述实体间关系的数据结构，很多涉及关系的计算问题，都可以转化为一个面向图的计算问题。比如，在社交网络分析、推荐网络分析、疾病传播探究、基因表达网络分析、细胞相似性分析等领域 图神经网络的本质 图神经网络是机器学习的一种特定方式，是神经网络在图数据应用上的一个自然延伸。 所谓机器学习，在形式上，可近似等同于通过统计或推理的方法寻找一个有关特定输入和预期输出的功能函数f，即 Y≈f(X)Y\\approx f(X)Y≈f(X)，图神经网络也是如此。 图数据处理面临的挑战 欧氏空间难表示图 传统的深度学习模型非常擅长处理简单而有序的序列数据或栅格数据，如常见的图像、音频、语音和文本等。 文本数据和语音数据具有一定的时序性，属于1D栅格数据，这种序列结构与循环神经网络(Recurrent Neural Network,RNN)的“品性”非常契合，因此RNN和LSTM在在具备时序特征的1D栅格数据处理上具有天然优势。 静态的图像属于2D栅格数据，动态的视频属于3D栅格数据（或者说是具有时序特征的2D栅格数据），它们非常适配于卷积神经网络(Convolutional Neural Network,CNN)模型。 图的表示要复杂得多，图之间的相似性也很难在欧氏空间中衡量。因此，人们不得不寻求在非欧氏空间中来定义图。 图表达无固定格式：图中的关系并没有固定的表达方式，这就导致很多完全等价的图同构(Graph Isomorphism)难以被发现。 图可视化难理解 图数据不符合独立同分布：独立同分布是机器学习领域很重要的假设。这个假设意味着，如果假设训练数据和测试数据是满足相同分布的，那么通过训练数据获得的模型（比如，拟合一个决策平面）能够在测试集上获得很好的预测效果。 图神经网络的应用层面 针对图数据，图神经网络的预测主要体现在三个层面：节点层面(Node-level)、边层面(Edge-level)、图层面(Graph-level)。 节点预测 在节点层面的预测任务主要包括分类任务和回归任务。分类任务和回归任务本质一样，类似于定性和定量的区别。 “知己知彼”，即（采样）聚合邻居节点的信息，并和自身节点信息进行融合。当邻居信息汇集到本地节点后，就可以利用传统的方法（如深度学习）来做节点的分类或回归 边预测 推荐系统 药物研发：研究药物-靶标相互作用、蛋白质-蛋白质相互作用和药物-药物相互作用 图预测 预测整个图的属性，将标签和整个图像关联起来 图神经网络的发展简史 早期的图神经网络 首先将图结构数据“映射”为更简单的表示，如边列表法，邻接列表法。 再用特定算法加以处理，如搜索算法（BFS，DFS）、最短路径（迪杰斯特拉算法）、生成树算法（Prim）以及聚类算法。 弊端： 可能丢失了图的结构信息，如节点间的拓扑关系 需要一定置信水平的关于图的先验知识 图卷积神经网络 卷积神经网络CNN开始在计算机视觉和图像处理领域“乘风破浪”，取得了很多令人瞩目的成就。CNN的显著特点：局部连接、权值共享、多层处理（通过“卷积”操作，提取多尺度局部空间特征，并将其组合，构建具有高度表达性的表示形式） 将CNN运用于Graph的理论基础： 图天然具备局部连通性质 与传统的谱图理论(Spectral Graph Theory)相比，权值共享能大幅降低训练成本 图本身具有层次属性，多层结构能够捕捉不同抽象级别的特征 将CNN运用于Graph的理论障碍 经典的CNN模型只适用于文本（1D栅格）、图像（2D栅格）等欧氏空间下的数据，而图通常被认为是非欧氏空间下的数据。 CNN中的卷积和汇聚，在图数据中难以定义 图卷积神经网络(Graph Convolutional Neural Network,GCN) 面向图数据的基于谱域(Spectral-Based)的卷积神经网络 拉普拉斯矩阵可视为图结构数据的一种表达方式。通过对拉普拉斯算子做特征分解，然后只取低阶的向量，可达到“低通滤波”的效果。 图卷积运算的一个极简版操作就是取中心节点及其邻居节点的特征平均值 基于频域的卷积，在计算时，需要在处理整个图数据的同时承担矩阵分解时的极高计算复杂度，当图数据的规模较大时，这种策略难以奏效 图表示学习 图表示学习旨在学习通过低维向量表征图的节点、边、子图。在图分析领域中，传统的机器学习方法往往依赖于手工设计的特征，这些方法灵活度较低且计算开销较大。于是图嵌入表示应运而生。 图嵌入(Graph Embedding)：将图中的节点以低维稠密向量的形式进行表达，要求原始图中相似的节点在低维表达空间中也是接近的。得到的低维表达向量可以用来完成下游任务，如节点分类、链接预测或重构原始图等。 DeepWalk：第一个基于表示学习的图嵌入方法，借鉴Word2Vec算法。简单且易于实现，但它也有一些局限性，比如它无法捕获节点之间的多跳关系。 随机游走（Random Walk） 从图中的某个节点开始，随机选择一个邻居节点进行移动。 重复该过程，每次都从前一个节点的邻居中随机选择一个节点，直到达到预定的路径长度。 生成节点序列 对图中的每个节点都执行多次随机游走，以生成大量的节点序列。 这些序列可以被视为“句子”，其中的节点可以被视为“单词”。 训练节点向量 使用节点序列，采用类似于Word2Vec中的Skip-Gram模型来训练节点的向量表示。 目标是使得图中相邻的节点在向量空间中具有相似的向量表示。 Line： Node2Vec：采用的是一种有偏(Bias)的随机游走，它可以看作DeepWalk算法的一种扩展，通过DFS（深度优先搜索）和BFS（广度优先搜索）策略，Node2vec方法能更好地发现图的结构（社区）信息，从而获得优于DeepWalk算法的性能。 图卷积的简化 谱域图卷积 GraphSAGE（Graph Sample and Aggregation） 通过采样中心节点的邻居，并对这些邻居的特征进行聚合，以此来更新中心节点的嵌入。 这个过程可以迭代多次，每次迭代都可以考虑更远的邻居 传统GCN通常需要处理整个图或者至少是一个较大的子图。 图神经网络的计算模块 传播模块(Propagation Module)。传播模块用于在节点之间传播信息，以便聚合邻居节点的特征信息和拓扑信息。在传播模块中，卷积算子和递归算子通常用于融合来自邻居的信息。在传播模块中，可采用跳跃连接(Skip Connection)结构，跳跃连接也称为“短路连接”，在残差网络(ResNet)中得到广泛应用。跳跃连接能从前向节点中直接抽取历史数据，从而防止信息在传播过程中过度衰减。 采样模块(Sampling Module)。GNN通常会聚合前一层节点的邻域信息，从而生成当前节点的表征。如果我们的汇聚操作涉及若干GNN层，那么计算牵涉的邻居节点数将会随着深度的增加而呈指数级增长。因此，人们通过采样技术来解决这种“邻居爆炸”的问题。当图比较大时，由于计算负载和存储的限制，也需要用采样模块对图的规模进行缩减。采样模块通常与传播模块配合使用。 汇聚模块(Pooling Module)。所谓汇聚（亦称池化），可形象地将其理解为“群众代表”，它用局部区间的代表性统计信息（比如最大值、最小值或平均值）来代替整个局部区间，从而减少待处理的数据量。通过层层汇聚，经过KKK轮消息传递，当前节点能够提取更为全局的信息 在每一个隐含层，我们都可以使用卷积/循环算子、采样算子和池化算子，或者添加若干汇聚模块用以减少数据量和提取更为高层的抽象信息。为了获得更好的性能，GNN层通常会堆叠多个隐含层。 图神经网络的分类 按照图类型的不同： 有向图 异构图(Heterogeneous Graphs) 带边信息的图(Edge-Informative Graph) 动态图(Dynamic Graphs) 训练过程中采取的局部数据处理方式的不同，主要存在于： 邻居节点采样 感受野(Receptive Field)（（如控制变量法）） 数据增强(Data Augmentation) 非监督训练方式（如利用不同的Boosting方法） 按照信息传播方式的不同： 展望 健壮性强的、有意识的、能决策的新一代人工智能 GNN可视为符号主义和联结主义的交叉融合。 它通过图节点之间的消息传递来捕获图的依赖性，这种依赖性可以用来传递因果关系、逻辑推理等（这是符号主义的体现）。 汇聚信息之后，对于处理信息的方式，中心节点完全可以用普通的神经网络来进行处理（这是联结主义的体现）。 近年来，网络架构、优化技术和并行计算等技术的发展推动了GNN的研究进程。特别是基于图神经网络的变体，如图卷积网络(GCN)、图注意网络(GAT)、门控图神经网络(GGNN)的提出，让GNN在许多任务上都取得了令人瞩目的成就。","categories":[{"name":"从深度学习到图神经网络","slug":"从深度学习到图神经网络","permalink":"https://minteen.github.io/categories/%E4%BB%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}],"tags":[]},{"title":"Linux常用命令","slug":"计算机基础/Linux/Linux常用命令","date":"2023-01-12T10:08:00.000Z","updated":"2024-02-16T12:42:36.881Z","comments":true,"path":"2023/01/12/计算机基础/Linux/Linux常用命令/","permalink":"https://minteen.github.io/2023/01/12/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/Linux/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","excerpt":"","text":"参考 ls: list ls -a 列出目录所有文件，包含以.开始的隐藏文件 ls -A 列出除.及…的其它文件 ls -r 反序排列 ls -t 以文件修改时间排序 ls -S 以文件大小排序 ls -h 以易读大小显示 ls -l 除了文件名之外，还将文件的权限、所有者、文件大小等信息详细列出来 ls -l t* 列出当前目录中所有以&quot;t&quot;开头的目录的详细内容 ls -lhrt 按易读方式按时间反序排序，并显示文件详细信息 ls | sed “s:^:`pwd`:” 列出文件绝对路径（不包含隐藏文件） find $pwd -maxdepth 1 | xargs ls -ld 列出文件绝对路径（包含隐藏文件） cd: changeDirectory cd ~ cd - 进入上一次工作路径 cd !$ 把上个命令的参数作为cd参数使用 pwd pwd -P 查看软链接的实际路径 mkdir rm rm – -f* 删除以 -f 开头的文件 rmdir mv mv * …/ 移动当前文件夹下的所有文件到上一级目录 cp -i 提示 -r 复制目录及目录内所有项目 -a 复制的文件与原文件时间一样 cat cat filename 一次显示整个文件 cat &gt; filename 从键盘创建一个文件 cat file1 file2 &gt; file 将几个文件合并为一个文件 more 功能类似于 cat, more 会以一页一页的显示方便使用者逐页阅读，而最基本的指令就是按空白键（space）就往下一页显示，按 b 键就会往回（back）一页显示。 less less 与 more 类似，但使用 less 可以随意浏览文件，而 more 仅能向前移动，却不能向后移动，而且 less 在查看之前不会加载整个文件。 tail 用于显示指定文件末尾内容，不指定文件时，作为输入信息进行处理。常用查看日志文件 ping 127.0.0.1 &gt; ping.log &amp; tail -f ping.log head which 查看可执行文件的位置 whereis 只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s） whereis -s locate 查找 locate 的源码文件 whereis -m locate 查找 lcoate 的帮助文件 locate find chmod tar chown df du ln date cal grep wc ps top kill free","categories":[{"name":"Linux","slug":"Linux","permalink":"https://minteen.github.io/categories/Linux/"}],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2022-12-31T17:12:12.000Z","updated":"2024-02-16T11:25:18.242Z","comments":true,"path":"2023/01/01/hello-world/","permalink":"https://minteen.github.io/2023/01/01/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"python基础","slug":"python基础","permalink":"https://minteen.github.io/categories/python%E5%9F%BA%E7%A1%80/"},{"name":"从深度学习到图神经网络","slug":"从深度学习到图神经网络","permalink":"https://minteen.github.io/categories/%E4%BB%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%B0%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"Linux","slug":"Linux","permalink":"https://minteen.github.io/categories/Linux/"}],"tags":[]}